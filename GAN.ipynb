{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DV2607 Project Notebook\n",
    "### Authors:\n",
    "### Oliver Ljung (ollj19@student.bth.se)\n",
    "### Phoebe Waters (phaa19@student.bth.se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, Flatten, Dense, Input, Activation, BatchNormalization, LeakyReLU, Reshape, UpSampling2D, Dropout, ReLU\n",
    "from keras import Sequential, Model\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as KB\n",
    "\n",
    "from keras.losses import BinaryCrossentropy, CategoricalCrossentropy, Hinge, SquaredHinge, MeanSquaredError, Loss, SparseCategoricalCrossentropy\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import art\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, CarliniL2Method\n",
    "from art.estimators.classification import KerasClassifier\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions\n",
    "\n",
    "def display_attack(x_test, x_adv_test, model):\n",
    "    x_real = x_test[:9]\n",
    "\n",
    "    x_fake = x_adv_test[:9]\n",
    "    x_fake_labels = model.predict(x_fake)\n",
    "    x_real_labels = model.predict(x_real)\n",
    "\n",
    "    for i in range(9):\n",
    "        fig = plt.subplot(3, 3, i+1)\n",
    "        fig.imshow(x_fake[i], cmap=plt.get_cmap('gray'))\n",
    "        print(f'p_fake = {np.argmax(x_fake_labels[i])}, p_real = {np.argmax(x_real_labels[i])}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining models\n",
    "\n",
    "def create_generator(img_shape, seed_shape, channels):\n",
    "    # Create a CNN model\n",
    "    model = Sequential(name=\"generator\")\n",
    "\n",
    "    model.add(Dense(img_shape[0]*img_shape[1]*img_shape[2], activation=\"relu\", input_dim=seed_shape))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, (3,3), activation='relu', padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (3,3), activation='relu', padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2D(channels, (3,3), activation='relu', padding=\"same\"))\n",
    "    model.add(Activation('tanh', name=\"generated_img\"))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    input = Input(shape = (seed_shape,))\n",
    "    \n",
    "    img = model(input)\n",
    "    \n",
    "    return Model(input, img, name=\"generator\")\n",
    "\n",
    "def create_discriminator(img_shape, verbose=True):\n",
    "    # Create a CNN model\n",
    "    model = Sequential(name=\"discriminator\")\n",
    "\n",
    "    # Add Convolution layers\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', input_shape=img_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add predictive layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid', name=\"validity_output\"))\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    # Add input\n",
    "    input = Input(shape = img_shape)\n",
    "\n",
    "    # Get ouput from model\n",
    "    validity = model(input)\n",
    "\n",
    "    # return model\n",
    "    return Model(input, validity, name=\"discriminator\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "train_X = train_X.astype(\"float32\") / 255\n",
    "test_X = test_X.astype(\"float32\") / 255\n",
    "\n",
    "train_X = np.expand_dims(train_X, -1)\n",
    "test_X = np.expand_dims(test_X, -1)\n",
    "\n",
    "train_y = to_categorical(train_y)\n",
    "train_y = np.array([np.argmax(y) for y in train_y])\n",
    "test_y  = to_categorical(test_y)\n",
    "test_y = np.array([np.argmax(y) for y in test_y])\n",
    "\n",
    "IMG_SHAPE = (28,28,1)\n",
    "CHANNELS = 1\n",
    "SEED_SHAPE = 100\n",
    "NUM_CLASSES = 10\n",
    "IMG_LOW_LIMIT = 0\n",
    "IMG_HIGH_LIMIT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# (train_X, train_y), (test_X, test_y) = cifar10.load_data()\n",
    "\n",
    "# train_X = train_X.astype(\"float32\")/255\n",
    "# test_X = test_X.astype(\"float32\")/255\n",
    "\n",
    "# train_y = to_categorical(train_y)\n",
    "# train_y = np.array([np.argmax(y) for y in train_y])\n",
    "# test_y  = to_categorical(test_y)\n",
    "# test_y = np.array([np.argmax(y) for y in test_y])\n",
    "\n",
    "# IMG_SHAPE = (32,32,3)\n",
    "# CHANNELS = 3\n",
    "# SEED_SHAPE = 100\n",
    "# NUM_CLASSES = 10\n",
    "# IMG_LOW_LIMIT = 0\n",
    "# IMG_HIGH_LIMIT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining models and input tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "input = Input(shape=(SEED_SHAPE,), name=\"seed\")\n",
    "\n",
    "generator = create_generator(IMG_SHAPE, SEED_SHAPE, CHANNELS)\n",
    "fake_image = generator(input)\n",
    "\n",
    "fake_image = tf.clip_by_value(fake_image, IMG_LOW_LIMIT, IMG_HIGH_LIMIT) # Values in image is [0,1]\n",
    "\n",
    "# We want the generator and discriminator to be trained in a combined model but as seperate entities\n",
    "discriminator = create_discriminator(IMG_SHAPE)\n",
    "discriminator.compile(optimizer=optimizer, loss=BinaryCrossentropy(), metrics=[\"accuracy\"])\n",
    "discriminator.trainable = False\n",
    "validity_fake = discriminator(fake_image)\n",
    "\n",
    "GAN_model = Model(inputs=input, outputs=validity_fake, name=\"GAN-net\")\n",
    "\n",
    "GAN_model.compile(optimizer=optimizer, loss=BinaryCrossentropy(), metrics=[\"accuracy\"])\n",
    "GAN_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 \n",
    "EPOCHS = 1_000_000\n",
    "\n",
    "y_real = np.ones((BATCH_SIZE, 1))\n",
    "y_fake = np.zeros((BATCH_SIZE, 1))\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    if time.time() - start_time > 60*60*3:\n",
    "        break   # time limit of 3 hours\n",
    "\n",
    "    x_real = np.array(random.choices(train_X, k=BATCH_SIZE))\n",
    "    seed = np.random.normal(0,1,(BATCH_SIZE, SEED_SHAPE))\n",
    "    \n",
    "    x_fake = generator.predict(seed, verbose=0)\n",
    "    x_fake = np.clip(x_fake, IMG_LOW_LIMIT, IMG_HIGH_LIMIT)\n",
    "    \n",
    "    discriminator_loss_real = discriminator.train_on_batch(x=x_real, y=y_real)\n",
    "    discriminator_loss_fake = discriminator.train_on_batch(x=x_fake, y=y_fake)\n",
    "    discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
    "\n",
    "    generator_loss = GAN_model.train_on_batch(x=seed, y=y_real)\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"EPOCH: {epoch}\")\n",
    "        print(f\"    Generator Loss:     {round(generator_loss[0], 3)},  Discriminator Loss:     {round(discriminator_loss[0], 3)}\")\n",
    "        print(f\"    Generator Accuracy: {round(generator_loss[1], 3)},  Discriminator Accuracy: {round(discriminator_loss[1], 3)}\")\n",
    "\n",
    "        for i in range(3):\n",
    "            fig = plt.subplot(1, 3, i+1)\n",
    "            fig.imshow(x_fake[i], cmap=plt.get_cmap('gray'))\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33a4542e4cc6c10be1951f7181ece9e030a5d84a8de853c4e052a2492c0e6b9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
