{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DV2607 Project Notebook\n",
    "### Authors:\n",
    "### Oliver Ljung (ollj19@student.bth.se)\n",
    "### Phoebe Waters (phaa19@student.bth.se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, Flatten, Dense, Input, Activation, BatchNormalization, LeakyReLU, Reshape, UpSampling2D, Dropout, ReLU\n",
    "from keras import Sequential, Model\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as KB\n",
    "\n",
    "from keras.losses import BinaryCrossentropy, CategoricalCrossentropy, Hinge, SquaredHinge, MeanSquaredError, Loss, SparseCategoricalCrossentropy\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import art\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, CarliniL2Method, CarliniL0Method\n",
    "from art.estimators.classification import KerasClassifier\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()      # Enable when training NN but has to be disabled for art\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions\n",
    "\n",
    "def display_attack(x_test, x_adv_test, model):\n",
    "    x_real = x_test[:9]\n",
    "\n",
    "    x_fake = x_adv_test[:9]\n",
    "    x_fake_labels = model.predict(x_fake)\n",
    "    x_real_labels = model.predict(x_real)\n",
    "\n",
    "    for i in range(9):\n",
    "        fig = plt.subplot(3, 3, i+1)\n",
    "        fig.imshow(x_fake[i], cmap=plt.get_cmap('gray'))\n",
    "        print(f'p_fake = {np.argmax(x_fake_labels[i])}, p_real = {np.argmax(x_real_labels[i])}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining models\n",
    "\n",
    "def create_classifier_alt(img_shape, num_of_classes, verbose=True):\n",
    "    # Create a CNN model\n",
    "\n",
    "    encoder = Sequential(name=\"classifier_encoder\")\n",
    "\n",
    "    # Add Convolution layers\n",
    "    encoder.add(Conv2D(32, (3,3), activation='relu', padding=\"same\",  input_shape=img_shape, name=\"image_input\"))\n",
    "    encoder.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    encoder.add(Conv2D(64, (3,3), activation='relu', padding=\"same\"))\n",
    "    encoder.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    encoder.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    encoder.add(BatchNormalization(momentum=0.8))\n",
    "    encoder.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # Add residual layer\n",
    "    resblock = Sequential(name=\"classifier_resblock\")\n",
    "    resblock.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    resblock.add(BatchNormalization(momentum=0.8))\n",
    "    resblock.add(ReLU())\n",
    "\n",
    "    resblock.add(Dropout(0.25))\n",
    "\n",
    "    resblock.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    resblock.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "    resblock.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    resblock.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "    # Add decoder\n",
    "    decoder = Sequential(name=\"classifier_decoder\")\n",
    "    decoder.add(Flatten())\n",
    "    decoder.add(Dense(1024, activation='relu'))\n",
    "    decoder.add(Dense(256, activation='relu'))\n",
    "    decoder.add(Dense(64, activation='relu'))\n",
    "    decoder.add(Dense(num_of_classes, activation=None, name=\"classication_output\"))\n",
    "\n",
    "    # Combining layers to model\n",
    "    input = Input(shape = img_shape)\n",
    "    encoding = encoder(input)\n",
    "    bottleneck = resblock(encoding) + encoding\n",
    "    bottleneck = resblock(bottleneck) + bottleneck\n",
    "    bottleneck = resblock(bottleneck) + bottleneck\n",
    "    prediction = decoder(bottleneck)\n",
    "    \n",
    "    model = Model(input, prediction, name=\"classifier\")\n",
    "\n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "\n",
    "    # return model\n",
    "    return model\n",
    "\n",
    "def create_classifier(img_shape, num_of_classes):\n",
    "    # Create a CNN model\n",
    "    # Add input\n",
    "    input = Input(shape = img_shape)\n",
    "\n",
    "    model = Sequential(name=\"mnist_classifier\")\n",
    "\n",
    "    # Add Convolution layers\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', input_shape=img_shape, name=\"image_input\"))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add predictive layers\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "\n",
    "    model.add(Dense(num_of_classes, activation=None, name=\"classication_output\"))\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    # Get ouput from model\n",
    "    prediction = model(input)\n",
    "    # prediction = tf.clip_by_value(prediction, 0, 1)\n",
    "\n",
    "    # return model\n",
    "    return Model(input, prediction, name=\"classifier\")\n",
    "\n",
    "def create_resblock():\n",
    "    resblock = Sequential()\n",
    "    resblock.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    resblock.add(BatchNormalization(momentum=0.8))\n",
    "    resblock.add(ReLU())\n",
    "\n",
    "    resblock.add(Dropout(0.25))\n",
    "\n",
    "    resblock.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    resblock.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "    resblock.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    resblock.add(BatchNormalization(momentum=0.8))\n",
    "    resblock.add(ReLU())\n",
    "\n",
    "    resblock.add(Dropout(0.25))\n",
    "\n",
    "    resblock.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    resblock.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "    resblock.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    resblock.add(BatchNormalization(momentum=0.8))\n",
    "    resblock.add(ReLU())\n",
    "\n",
    "    resblock.add(Dropout(0.25))\n",
    "\n",
    "    resblock.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    resblock.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "    return resblock\n",
    "\n",
    "def create_adv_generator(img_shape, channels, boundary, verbose=True):\n",
    "    # Create a CNN model\n",
    "\n",
    "    # Enocder\n",
    "    encoder = Sequential(name=\"adv_generator_encoder\")\n",
    "    encoder.add(Conv2D(32, (3,3), activation='relu', padding=\"same\", input_shape=img_shape, name=\"image_input\"))\n",
    "    encoder.add(BatchNormalization(momentum=0.8))\n",
    "    encoder.add(UpSampling2D())\n",
    "    \n",
    "    encoder.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    encoder.add(BatchNormalization(momentum=0.8))\n",
    "    encoder.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # Bottle neck w residual block\n",
    "    resblock1 = create_resblock()\n",
    "    resblock2 = create_resblock()\n",
    "    resblock3 = create_resblock()\n",
    "\n",
    "    # Decoder\n",
    "    decoder = Sequential(name=\"adv_generator_decoder\")\n",
    "    decoder.add(Conv2DTranspose(32, (3,3), activation='relu', padding=\"same\"))\n",
    "    decoder.add(BatchNormalization(momentum=0.8))\n",
    "    decoder.add(UpSampling2D())\n",
    "\n",
    "    decoder.add(Conv2DTranspose(64, (3,3), padding=\"same\"))\n",
    "    decoder.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    decoder.add(MaxPooling2D((4, 4)))\n",
    "    decoder.add(Conv2DTranspose(channels, (3,3), padding=\"same\"))\n",
    "    decoder.add(Activation('tanh', name=\"adv_image_output\"))\n",
    "    \n",
    "    # Combining layers to model\n",
    "    input = Input(shape = img_shape)\n",
    "    encoding = encoder(input)\n",
    "    bottleneck = resblock1(encoding) + encoding\n",
    "    bottleneck = resblock2(bottleneck) + bottleneck\n",
    "    bottleneck = resblock3(bottleneck) + bottleneck\n",
    "    perturbations = decoder(bottleneck)\n",
    "\n",
    "    perturbations = tf.clip_by_value(perturbations, -1, 1)\n",
    "    perturbations = tf.clip_by_value(perturbations, -boundary, boundary)\n",
    "    \n",
    "    model = Model(input, perturbations, name=\"adv_generator\")\n",
    "\n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "    \n",
    "    # return model\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_discriminator(img_shape, verbose=True):\n",
    "    # Create a CNN model\n",
    "    model = Sequential(name=\"discriminator\")\n",
    "\n",
    "    # Add Convolution layers\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', input_shape=img_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add predictive layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid', name=\"validity_output\"))\n",
    "    \n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "\n",
    "    # Add input\n",
    "    input = Input(shape = img_shape)\n",
    "\n",
    "    # Get ouput from model\n",
    "    validity = model(input)\n",
    "\n",
    "    # return model\n",
    "    return Model(input, validity, name=\"discriminator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "train_X = train_X.astype(\"float32\") / 255\n",
    "test_X = test_X.astype(\"float32\") / 255\n",
    "\n",
    "train_X = np.expand_dims(train_X, -1)\n",
    "test_X = np.expand_dims(test_X, -1)\n",
    "\n",
    "train_y = to_categorical(train_y)\n",
    "train_y = np.array([np.argmax(y) for y in train_y])\n",
    "test_y  = to_categorical(test_y)\n",
    "test_y = np.array([np.argmax(y) for y in test_y])\n",
    "\n",
    "IMG_SHAPE = (28,28,1)\n",
    "CHANNELS = 1\n",
    "NUM_CLASSES = 10\n",
    "IMG_LOW_LIMIT = 0\n",
    "IMG_HIGH_LIMIT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# (train_X, train_y), (test_X, test_y) = cifar10.load_data()\n",
    "\n",
    "# train_X = train_X.astype(\"float32\")/255\n",
    "# test_X = test_X.astype(\"float32\")/255\n",
    "\n",
    "# train_y = to_categorical(train_y)\n",
    "# train_y = np.array([np.argmax(y) for y in train_y])\n",
    "# test_y  = to_categorical(test_y)\n",
    "# test_y = np.array([np.argmax(y) for y in test_y])\n",
    "\n",
    "# IMG_SHAPE = (32,32,3)\n",
    "# CHANNELS = 3\n",
    "# NUM_CLASSES = 10\n",
    "# IMG_LOW_LIMIT = 0\n",
    "# IMG_HIGH_LIMIT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(test_X[i], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Classifier; model to attack and later defend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam()\n",
    "# optimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "\n",
    "model = create_classifier(IMG_SHAPE, NUM_CLASSES)\n",
    "model.compile(optimizer=optimizer, loss=SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_X, train_y, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining models and input tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Values for advGAN ###\n",
    "# MNIST\n",
    "# alpha = 1\n",
    "# beta = 1\n",
    "# c = 0.7\n",
    "\n",
    "# CIFAR-10\n",
    "alpha = 1\n",
    "beta = 1\n",
    "c = 0.8\n",
    "#########################\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "input = Input(shape=IMG_SHAPE, name=\"image\")\n",
    "\n",
    "adv_generator = create_adv_generator(IMG_SHAPE, CHANNELS, c)\n",
    "perturbations = adv_generator(input)\n",
    "\n",
    "adv_image = tf.add(input, perturbations)\n",
    "adv_image = tf.clip_by_value(adv_image, IMG_LOW_LIMIT, IMG_HIGH_LIMIT)\n",
    "\n",
    "# We want the generator and discriminator to be trained in a combined model but as seperate entities\n",
    "discriminator = create_discriminator(IMG_SHAPE)\n",
    "discriminator.compile(optimizer=optimizer, loss=BinaryCrossentropy(), metrics=[\"accuracy\"])\n",
    "discriminator.trainable = False\n",
    "validity_adv = discriminator(adv_image)\n",
    "\n",
    "\n",
    "# We dont want to train our model to attack\n",
    "model.trainable = False\n",
    "prediction = model(adv_image)\n",
    "\n",
    "outputs_w_names = {\n",
    "    \"classifier\": prediction,\n",
    "    \"discriminator\": validity_adv,\n",
    "    \"adv_generator\": perturbations,\n",
    "}\n",
    "\n",
    "advGAN_model = Model(inputs=input, outputs=outputs_w_names, name=\"advGAN-net\")\n",
    "\n",
    "class PerturbationLoss(Loss):\n",
    "    # Lhinge = Ex max(0, kG(x)k2 âˆ’ c)\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Colin Targonski (ctargon), Feb-2019, link: https://github.com/ctargon/AdvGAN-tf/blob/master/AdvGAN.py\n",
    "        zeros = tf.zeros((tf.shape(perturbations)[0]))\n",
    "        L_hinge = tf.reduce_mean(tf.maximum(zeros, tf.norm(tf.reshape(y_pred, (tf.shape(y_pred)[0], -1)), axis=1) - c))\n",
    "        return L_hinge\n",
    "\n",
    "L_adv = SparseCategoricalCrossentropy(from_logits=True)\n",
    "L_GAN = BinaryCrossentropy()\n",
    "L_hinge = PerturbationLoss()\n",
    "\n",
    "losses = {\n",
    "    \"classifier\": L_adv,\n",
    "    \"discriminator\": L_GAN,\n",
    "    \"adv_generator\": L_hinge,\n",
    "}\n",
    "\n",
    "losses_weights = {\n",
    "    \"classifier\": 1,\n",
    "    \"discriminator\": alpha,\n",
    "    \"adv_generator\": beta,\n",
    "}\n",
    "\n",
    "advGAN_model.compile(optimizer=optimizer, loss=losses, loss_weights=losses_weights, metrics=[\"accuracy\"])\n",
    "advGAN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdvGAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8 \n",
    "EPOCHS = 100_000\n",
    "\n",
    "y_real = np.ones((BATCH_SIZE, 1))\n",
    "y_fake = np.zeros((BATCH_SIZE, 1))\n",
    "\n",
    "# Targeted Attack\n",
    "\n",
    "target_prediction = np.zeros([BATCH_SIZE, 1])\n",
    "\n",
    "hinge_limit = np.array([[c]]*BATCH_SIZE)\n",
    "\n",
    "discriminator_acc_history = []\n",
    "classifier_acc_history = []\n",
    "max_discriminator_acc = 0\n",
    "max_classifier_acc = 0.4\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(EPOCHS+1):\n",
    "    if time.time() - start_time > 60*60*10:\n",
    "        break   # time limit of 10 hours\n",
    "\n",
    "    x_real = np.array(random.choices(train_X, k=BATCH_SIZE))\n",
    "    \n",
    "    perturbations = adv_generator.predict(x_real, verbose=0)\n",
    "\n",
    "    x_fake = x_real + perturbations\n",
    "    x_fake = np.clip(x_fake, IMG_LOW_LIMIT, IMG_HIGH_LIMIT)\n",
    "\n",
    "    y = {\n",
    "        \"classifier\": target_prediction,\n",
    "        \"discriminator\": y_real,\n",
    "        \"adv_generator\": perturbations\n",
    "    }\n",
    "\n",
    "    # Evaluate how successful the attack is\n",
    "    classifier_loss = model.evaluate(x_fake, target_prediction, verbose=False)\n",
    "\n",
    "    # We want our discriminator to learn about fakes after\n",
    "    discriminator_loss_real = discriminator.train_on_batch(x=x_real, y=y_real)\n",
    "    discriminator_loss_fake = discriminator.train_on_batch(x=x_fake, y=y_fake)\n",
    "    discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
    "\n",
    "    loss = advGAN_model.train_on_batch(x=x_real, y=y)\n",
    "\n",
    "    # Add values to history\n",
    "    discriminator_acc_history.append(discriminator_loss[1])\n",
    "    classifier_acc_history.append(classifier_loss[1])\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        if max_discriminator_acc < np.mean(discriminator_acc_history):\n",
    "            discriminator.save(\"mnist_adv_discriminator_t0.h5\")\n",
    "        if max_classifier_acc < np.mean(classifier_acc_history):         \n",
    "            adv_generator.save(\"mnist_adv_generator_t0.h5\")\n",
    "\n",
    "        print(f\"EPOCH: {epoch}\")\n",
    "        print(f\"Total loss = {loss[0]} \\n\\tLosses: adv_generator = {loss[1]}, classifier = {loss[2]}, discriminator = {loss[3]}\\n\\tAccuracy: adv_generator = {loss[4]}, mnist_classifier = {loss[5]}, discriminator = {loss[6]} \\n\")\n",
    "        print(f\"    Discriminator: Accuracy: {np.mean(discriminator_acc_history)}, Classifier: Accuracy: {np.mean(classifier_acc_history)}\")\n",
    "        \n",
    "        discriminator_acc_history = []\n",
    "        classifier_acc_history = []\n",
    "\n",
    "        for i in range(5):\n",
    "            fig = plt.subplot(2, 5, i+1)\n",
    "            fig.imshow(x_fake[i], cmap=plt.get_cmap('gray'))\n",
    "        for i in range(5):\n",
    "            fig = plt.subplot(2, 5, i+6)\n",
    "            fig.imshow(x_real[i], cmap=plt.get_cmap('gray'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use same models as in paper\n",
    "\n",
    "model = tf.keras.models.load_model(\"models/mnist_classifier__report.h5\")\n",
    "discriminator = tf.keras.models.load_model(\"models/mnist_adv_discriminator_t0__report.h5\")      \n",
    "adv_generator = tf.keras.models.load_model(\"models/mnist_adv_generator_t0__report.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbations = adv_generator.predict(test_X, verbose = 1)\n",
    "advGAN_test_X = np.add(test_X, perturbations)\n",
    "advGAN_test_X = np.clip(advGAN_test_X, 0, 1) # Values in image is [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(advGAN_test_X, np.zeros([len(advGAN_test_X), 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attack(test_X, advGAN_test_X, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to convert classifier model to art\n",
    "clf = KerasClassifier(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline for attack (% classified as 0s in test)\n",
    "model.evaluate(test_X, np.zeros(len(test_X)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method_attack(x_test, clf):\n",
    "    # MNIST params: eps=0.03, 10 steps\n",
    "    # CIFAR-10 params: eps=0.01, 10 steps\n",
    "    FGM = FastGradientMethod(clf, eps=0.03, targeted=True, batch_size=32)\n",
    "\n",
    "    # Targeted Attack\n",
    "    target_prediction = np.zeros([len(x_test), 1])\n",
    "\n",
    "    FGM_test_X = FGM.generate(x_test, target_prediction)\n",
    "    for step in range(10):\n",
    "        FGM_test_X = FGM.generate(FGM_test_X, target_prediction)\n",
    "\n",
    "    return FGM_test_X\n",
    "\n",
    "FGM_test_X = fast_gradient_method_attack(test_X, clf)\n",
    "model.evaluate(FGM_test_X, np.zeros(len(FGM_test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attack(test_X, FGM_test_X, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_descent_attack(x_test, clf):\n",
    "    # MNIST params: max_iter=10, eps_step=0.05, eps=0.3\n",
    "    # CIFAR-10 params: max_iter=10, eps_step=0.01, eps=0.1, \n",
    "    PGD = ProjectedGradientDescent(clf, targeted=True, max_iter=10, eps_step=0.05, eps=0.3, verbose=False)\n",
    "\n",
    "    # Targeted Attack\n",
    "    target_prediction = np.zeros([len(x_test), 1])\n",
    "\n",
    "    PGD_test_X = PGD.generate(x_test, target_prediction)\n",
    "\n",
    "    return PGD_test_X\n",
    "\n",
    "PGD_test_X = projected_gradient_descent_attack(test_X, clf)\n",
    "model.evaluate(PGD_test_X, np.zeros(len(PGD_test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attack(test_X, PGD_test_X, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Binary Input detector (discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryInputDetector():\n",
    "    def __init__(self, detector, target_model):\n",
    "        self._detector = detector\n",
    "        self._target_model = target_model\n",
    "    \n",
    "    def predict(self, array):\n",
    "        prediction = []\n",
    "        \n",
    "        for element in array:\n",
    "            if self._detector.predict(np.array([element]))[0][0] < 0.5:\n",
    "                # Advesary\n",
    "                prediction.append(-1)\n",
    "            else:\n",
    "                # Real\n",
    "                p = self._target_model.predict(np.array([element]))\n",
    "                p_id = np.argmax(p)\n",
    "                prediction.append(p_id)\n",
    "        \n",
    "        adv_rate = prediction.count(-1)/len(prediction)\n",
    "        return np.array(prediction), adv_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = tf.keras.models.load_model(\"models/mnist_discriminator__report.h5\")  # For normal GAN BID\n",
    "BID = BinaryInputDetector(discriminator, model)\n",
    "\n",
    "y_pred_BASE, adv_rate_BASE = BID.predict(test_X)\n",
    "y_pred_FGM, adv_rate_FGM = BID.predict(FGM_test_X)\n",
    "y_pred_PGD, adv_rate_PGD = BID.predict(PGD_test_X)\n",
    "y_pred_advGAN , adv_rate_advGAN  = BID.predict(advGAN_test_X)\n",
    "\n",
    "print(f\"Adv rates: BASE: {adv_rate_BASE}, FGM: {adv_rate_FGM}, PGD: {adv_rate_PGD}, advGAN: {adv_rate_advGAN}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33a4542e4cc6c10be1951f7181ece9e030a5d84a8de853c4e052a2492c0e6b9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
